{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47309c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Destination Port', 'Flow Duration', 'Total Fwd Packets', 'Total Length of Fwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'Average Packet Size', 'Subflow Fwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Max', 'Idle Min', 'Attack Type']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# csv_path = \"data/cicids2017_cleaned.csv\"   \n",
    "# df = pd.read_csv(csv_path)\n",
    "\n",
    "print(df.columns.tolist())\n",
    "label_cols = [c for c in df.columns if \"label\" in c.lower()]\n",
    "print(label_cols)\n",
    "for col in label_cols:\n",
    "    print(df[col].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f306f6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\ids\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.1+cu130\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851550ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Config & paths\n",
    "\n",
    "X_PATH = r\"processed\\preprocessed_X_seq.npy\"\n",
    "Y_PATH = r\"processed\\preprocessed_y_seq.npy\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "LR = 1e-3\n",
    "SEQ_LEN = 32\n",
    "\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15   # remaining 0.15 ‚Üí test\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SAVE_DIR = \"saved_models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edcb320b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_seq shape: (78773, 32, 52)\n",
      "y_seq shape: (78773, 32)\n",
      "Train windows: 55141\n",
      "Val windows: 11815\n",
      "Test windows: 11817\n",
      "Train label distribution (per-flow): {np.int64(0): np.int64(1532563), np.int64(1): np.int64(231949)}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load preprocessed windows & split into train/val/test\n",
    "\n",
    "X_seq = np.load(X_PATH)   # shape: (N_windows, 32, 52)\n",
    "y_seq = np.load(Y_PATH)   # shape: (N_windows, 32)\n",
    "\n",
    "print(\"X_seq shape:\", X_seq.shape)\n",
    "print(\"y_seq shape:\", y_seq.shape)\n",
    "\n",
    "def create_splits(X_seq, y_seq, train_ratio=0.7, val_ratio=0.15):\n",
    "    N = X_seq.shape[0]\n",
    "    n_train = int(N * train_ratio)\n",
    "    n_val = int(N * val_ratio)\n",
    "    n_test = N - n_train - n_val\n",
    "\n",
    "    X_train = X_seq[:n_train]\n",
    "    y_train = y_seq[:n_train]\n",
    "    X_val   = X_seq[n_train:n_train + n_val]\n",
    "    y_val   = y_seq[n_train:n_train + n_val]\n",
    "    X_test  = X_seq[n_train + n_val:]\n",
    "    y_test  = y_seq[n_train + n_val:]\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = create_splits(\n",
    "    X_seq, y_seq, TRAIN_RATIO, VAL_RATIO\n",
    ")\n",
    "\n",
    "print(\"Train windows:\", X_train.shape[0])\n",
    "print(\"Val windows:\",   X_val.shape[0])\n",
    "print(\"Test windows:\",  X_test.shape[0])\n",
    "\n",
    "# Show class balance in training set (flatten to flow-level)\n",
    "unique, counts = np.unique(y_train.reshape(-1), return_counts=True)\n",
    "print(\"Train label distribution (per-flow):\", dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f8eb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One batch shapes: torch.Size([32, 32, 52]) torch.Size([32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: PyTorch Dataset wrapping (X_seq, y_seq)\n",
    "\n",
    "class FlowSequenceDatasetFromArrays(Dataset):\n",
    "    \"\"\"\n",
    "    Each item is one window of flows:\n",
    "      x: (seq_len, num_features)\n",
    "      y: (seq_len,)\n",
    "    \"\"\"\n",
    "    def __init__(self, X_seq, y_seq):\n",
    "        assert X_seq.shape[0] == y_seq.shape[0]\n",
    "        self.X = X_seq\n",
    "        self.y = y_seq\n",
    "        self.num_windows, self.seq_len, self.num_features = self.X.shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_windows\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_win = torch.tensor(self.X[idx], dtype=torch.float32)  # (L, F)\n",
    "        y_win = torch.tensor(self.y[idx], dtype=torch.long)     # (L,)\n",
    "        return x_win, y_win\n",
    "\n",
    "train_ds = FlowSequenceDatasetFromArrays(X_train, y_train)\n",
    "val_ds   = FlowSequenceDatasetFromArrays(X_val, y_val)\n",
    "test_ds  = FlowSequenceDatasetFromArrays(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"One batch shapes:\", xb.shape, yb.shape)  # (B, L, F), (B, L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ddf582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: GraphSAGEEncoder (GNN branch) using a complete graph per window\n",
    "\n",
    "class GraphSAGEEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim=128, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_dim = emb_dim\n",
    "        for i in range(num_layers):\n",
    "            out_dim = hidden_dim if i < num_layers - 1 else emb_dim\n",
    "            layers.append(SAGEConv(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        self.convs = nn.ModuleList(layers)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def build_complete_edge_index(self, B, L, device):\n",
    "        \"\"\"\n",
    "        Build a complete directed graph (no self-loops)\n",
    "        for each of B windows, each with L nodes.\n",
    "        Returns edge_index of shape (2, E_total).\n",
    "        \"\"\"\n",
    "        idx = torch.arange(L, device=device)   # (L,)\n",
    "        src, dst = torch.meshgrid(idx, idx, indexing=\"ij\")  # (L, L)\n",
    "\n",
    "        # Remove self loops\n",
    "        mask = src != dst\n",
    "        src = src[mask].reshape(-1)  # (E_per_window,)\n",
    "        dst = dst[mask].reshape(-1)  # (E_per_window,)\n",
    "\n",
    "        edge_indices = []\n",
    "        for b in range(B):\n",
    "            offset = b * L\n",
    "            edge_indices.append(\n",
    "                torch.stack([src + offset, dst + offset], dim=0)  # (2, E_per_window)\n",
    "            )\n",
    "\n",
    "        edge_index = torch.cat(edge_indices, dim=1)  # (2, E_total)\n",
    "        return edge_index\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, L, emb_dim)\n",
    "        returns: (B, L, emb_dim)\n",
    "        \"\"\"\n",
    "        B, L, D = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        x_flat = x.reshape(B * L, D)  # (N, D) where N = B*L\n",
    "        edge_index = self.build_complete_edge_index(B, L, device)  # (2, E)\n",
    "\n",
    "        h = x_flat\n",
    "        for conv in self.convs:\n",
    "            h = conv(h, edge_index)\n",
    "            h = self.act(h)\n",
    "\n",
    "        h = h.reshape(B, L, D)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d48e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid model parameters: 457090\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Hybrid Transformer + GNN IDS model\n",
    "\n",
    "class HybridTransformerGNN_IDS(nn.Module):\n",
    "    def __init__(self, in_dim, emb_dim=128, n_heads=4, n_layers=2,\n",
    "                 num_classes=2, max_seq_len=32, gnn_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Shared flow embedding\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, emb_dim)\n",
    "        )\n",
    "\n",
    "        # Positional embeddings\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, emb_dim)\n",
    "\n",
    "        # Transformer encoder (temporal branch)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=256,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # GraphSAGE encoder (graph branch)\n",
    "        self.gnn = GraphSAGEEncoder(\n",
    "            emb_dim=emb_dim,\n",
    "            hidden_dim=128,\n",
    "            num_layers=gnn_layers\n",
    "        )\n",
    "\n",
    "        # Fusion MLP with dropout for better generalization\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, L, in_dim)\n",
    "        returns: (B, L, num_classes)\n",
    "        \"\"\"\n",
    "        B, L, _ = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # Shared flow embedding\n",
    "        x_emb = self.embed(x)  # (B, L, emb_dim)\n",
    "\n",
    "        # Transformer branch (temporal)\n",
    "        positions = torch.arange(L, device=device).unsqueeze(0).expand(B, L)\n",
    "        temp_in = x_emb + self.pos_emb(positions)\n",
    "        z_temp = self.transformer(temp_in)  # (B, L, emb_dim)\n",
    "\n",
    "        # GNN branch (relational)\n",
    "        z_graph = self.gnn(x_emb)          # (B, L, emb_dim)\n",
    "\n",
    "        # Fusion\n",
    "        z = torch.cat([z_temp, z_graph], dim=-1)  # (B, L, 2*emb_dim)\n",
    "        z = self.fusion(z)                        # (B, L, 128)\n",
    "\n",
    "        logits = self.classifier(z)               # (B, L, num_classes)\n",
    "        return logits\n",
    "\n",
    "# Instantiate once to see param count\n",
    "in_dim = X_seq.shape[-1]\n",
    "model = HybridTransformerGNN_IDS(in_dim=in_dim, emb_dim=128, n_heads=4, n_layers=2,\n",
    "                                 num_classes=2, max_seq_len=SEQ_LEN, gnn_layers=2)\n",
    "model.to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Hybrid model parameters:\", total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c53c5cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts (0,1): [1532563.  231949.]  -> class weights: [0.5756736 3.8036637]\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Helper functions ‚Äì accuracy, class weights\n",
    "\n",
    "def accuracy_from_logits(logits, targets):\n",
    "    \"\"\"\n",
    "    logits:  (B, L, C)\n",
    "    targets: (B, L)\n",
    "    \"\"\"\n",
    "    preds = torch.argmax(logits, dim=-1)  # (B, L)\n",
    "    correct = (preds == targets).float()\n",
    "    return correct.mean().item()\n",
    "\n",
    "def compute_class_weights_from_train(y_train, device):\n",
    "    \"\"\"\n",
    "    y_train: (N_windows, L) numpy array with 0/1 labels\n",
    "    Returns torch tensor of shape (2,) with inverse-frequency weights.\n",
    "    \"\"\"\n",
    "    flat = y_train.reshape(-1)\n",
    "    counts = np.bincount(flat, minlength=2).astype(np.float32)\n",
    "    total = counts.sum()\n",
    "    # Simple inverse-frequency weights, normalized\n",
    "    weights = total / (2.0 * counts + 1e-8)\n",
    "    w_tensor = torch.tensor(weights, device=device, dtype=torch.float32)\n",
    "    print(\"Class counts (0,1):\", counts, \" -> class weights:\", weights)\n",
    "    return w_tensor\n",
    "\n",
    "class_weights = compute_class_weights_from_train(y_train, DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd8f7ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ New best hybrid model saved (epoch 1) with Val Acc = 0.8267\n",
      "Epoch 1/15 | Train Loss: 0.0446, Train Acc: 0.9877 | Val Loss: 2.1831, Val Acc: 0.8267\n",
      "Epoch 2/15 | Train Loss: 0.0245, Train Acc: 0.9938 | Val Loss: 1.3798, Val Acc: 0.8171\n",
      "Epoch 3/15 | Train Loss: 0.0208, Train Acc: 0.9948 | Val Loss: 1.7646, Val Acc: 0.7642\n",
      "Epoch 4/15 | Train Loss: 0.0173, Train Acc: 0.9954 | Val Loss: 2.5332, Val Acc: 0.6777\n",
      "Epoch 5/15 | Train Loss: 0.0153, Train Acc: 0.9957 | Val Loss: 2.2169, Val Acc: 0.7407\n",
      "üíæ New best hybrid model saved (epoch 6) with Val Acc = 0.8369\n",
      "Epoch 6/15 | Train Loss: 0.0129, Train Acc: 0.9965 | Val Loss: 1.6175, Val Acc: 0.8369\n",
      "Epoch 7/15 | Train Loss: 0.0119, Train Acc: 0.9966 | Val Loss: 2.5539, Val Acc: 0.7349\n",
      "üíæ New best hybrid model saved (epoch 8) with Val Acc = 0.8732\n",
      "Epoch 8/15 | Train Loss: 0.0113, Train Acc: 0.9967 | Val Loss: 1.4894, Val Acc: 0.8732\n",
      "Epoch 9/15 | Train Loss: 0.0097, Train Acc: 0.9970 | Val Loss: 2.0873, Val Acc: 0.8136\n",
      "Epoch 10/15 | Train Loss: 0.0095, Train Acc: 0.9971 | Val Loss: 1.8544, Val Acc: 0.8648\n",
      "Epoch 11/15 | Train Loss: 0.0091, Train Acc: 0.9972 | Val Loss: 2.3223, Val Acc: 0.8661\n",
      "Epoch 12/15 | Train Loss: 0.0082, Train Acc: 0.9974 | Val Loss: 2.2795, Val Acc: 0.8634\n",
      "Epoch 13/15 | Train Loss: 0.0080, Train Acc: 0.9974 | Val Loss: 2.6855, Val Acc: 0.8412\n",
      "Epoch 14/15 | Train Loss: 0.0078, Train Acc: 0.9975 | Val Loss: 2.4037, Val Acc: 0.8666\n",
      "Epoch 15/15 | Train Loss: 0.0073, Train Acc: 0.9976 | Val Loss: 2.6034, Val Acc: 0.8635\n",
      "\n",
      "‚úÖ Hybrid training finished.\n",
      "Best validation accuracy: 0.8731539576440244\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Train hybrid model with class-weighted loss\n",
    "\n",
    "hybrid_model = HybridTransformerGNN_IDS(\n",
    "    in_dim=in_dim,\n",
    "    emb_dim=128,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    num_classes=2,\n",
    "    max_seq_len=SEQ_LEN,\n",
    "    gnn_layers=2,\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(hybrid_model.parameters(), lr=LR)\n",
    "\n",
    "# Optional: LR scheduler (based on validation loss)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2\n",
    ")\n",
    "\n",
    "best_val_acc = -1.0\n",
    "best_model_path = os.path.join(SAVE_DIR, \"hybrid_transformer_gnn_best_weighted.pth\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ---------- TRAIN ----------\n",
    "    hybrid_model.train()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(DEVICE)  # (B, L, F)\n",
    "        yb = yb.to(DEVICE)  # (B, L)\n",
    "\n",
    "        logits = hybrid_model(xb)  # (B, L, 2)\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.view(-1, logits.size(-1)),  # (B*L, C)\n",
    "            yb.view(-1)                        # (B*L,)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy_from_logits(logits, yb)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_train_acc  = total_acc / len(train_loader)\n",
    "\n",
    "    # ---------- VALIDATION ----------\n",
    "    hybrid_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "\n",
    "            logits = hybrid_model(xb)\n",
    "            loss = criterion(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                yb.view(-1)\n",
    "            )\n",
    "            val_loss += loss.item()\n",
    "            val_acc += accuracy_from_logits(logits, yb)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_acc  = val_acc / len(val_loader)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    if avg_val_acc > best_val_acc:\n",
    "        best_val_acc = avg_val_acc\n",
    "        torch.save(hybrid_model.state_dict(), best_model_path)\n",
    "        print(f\"üíæ New best hybrid model saved (epoch {epoch}) with Val Acc = {best_val_acc:.4f}\")\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f} | \"\n",
    "        f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n‚úÖ Hybrid training finished.\")\n",
    "print(\"Best validation accuracy:\", best_val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb6ca1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TEST METRICS at threshold 0.5 (Hybrid + GNN, weighted loss):\n",
      "Accuracy : 0.9674\n",
      "Precision: 0.9753\n",
      "Recall   : 0.7778\n",
      "F1-score : 0.8654\n",
      "\n",
      "üîç Threshold sweep:\n",
      "th=0.10 | Acc=0.9661, Prec=0.9064, Rec=0.8344, F1=0.8689\n",
      "th=0.20 | Acc=0.9686, Prec=0.9389, Rec=0.8203, F1=0.8756\n",
      "th=0.30 | Acc=0.9691, Prec=0.9555, Rec=0.8079, F1=0.8755\n",
      "th=0.40 | Acc=0.9683, Prec=0.9673, Rec=0.7912, F1=0.8704\n",
      "th=0.50 | Acc=0.9674, Prec=0.9753, Rec=0.7778, F1=0.8654\n",
      "th=0.60 | Acc=0.9663, Prec=0.9812, Rec=0.7641, F1=0.8591\n",
      "th=0.70 | Acc=0.9654, Prec=0.9860, Rec=0.7536, F1=0.8542\n",
      "th=0.80 | Acc=0.9615, Prec=0.9894, Rec=0.7220, F1=0.8348\n",
      "th=0.90 | Acc=0.9506, Prec=0.9925, Rec=0.6382, F1=0.7768\n",
      "\n",
      "‚≠ê Best F1-score 0.8756 achieved at threshold 0.20\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Test metrics at 0.5 threshold + threshold sweep\n",
    "\n",
    "# Reload best model\n",
    "hybrid_model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "hybrid_model.to(DEVICE)\n",
    "hybrid_model.eval()\n",
    "\n",
    "all_probs = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "\n",
    "        logits = hybrid_model(xb)  # (B, L, 2)\n",
    "        probs = torch.softmax(logits, dim=-1)[..., 1]  # P(attack)\n",
    "\n",
    "        all_probs.append(probs.cpu().numpy().reshape(-1))\n",
    "        all_targets.append(yb.cpu().numpy().reshape(-1))\n",
    "\n",
    "all_probs = np.concatenate(all_probs)    # shape: (N_flows,)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "def eval_at_threshold(th):\n",
    "    preds = (all_probs >= th).astype(int)\n",
    "    acc  = accuracy_score(all_targets, preds)\n",
    "    prec = precision_score(all_targets, preds, zero_division=0)\n",
    "    rec  = recall_score(all_targets, preds, zero_division=0)\n",
    "    f1   = f1_score(all_targets, preds, zero_division=0)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "# Default 0.5 threshold (for comparison)\n",
    "acc_05, prec_05, rec_05, f1_05 = eval_at_threshold(0.5)\n",
    "print(\"‚úÖ TEST METRICS at threshold 0.5 (Hybrid + GNN, weighted loss):\")\n",
    "print(f\"Accuracy : {acc_05:.4f}\")\n",
    "print(f\"Precision: {prec_05:.4f}\")\n",
    "print(f\"Recall   : {rec_05:.4f}\")\n",
    "print(f\"F1-score : {f1_05:.4f}\")\n",
    "\n",
    "# Sweep thresholds to find best F1 and best Recall\n",
    "thresholds = np.linspace(0.1, 0.9, 9)\n",
    "print(\"\\nüîç Threshold sweep:\")\n",
    "best_f1 = -1\n",
    "best_f1_th = None\n",
    "\n",
    "for th in thresholds:\n",
    "    acc, prec, rec, f1 = eval_at_threshold(th)\n",
    "    print(f\"th={th:.2f} | Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}\")\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_f1_th = th\n",
    "\n",
    "print(f\"\\n‚≠ê Best F1-score {best_f1:.4f} achieved at threshold {best_f1_th:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d42bf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported metrics to results/Hybrid.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hybrid</td>\n",
       "      <td>0.96743</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.777815</td>\n",
       "      <td>0.865434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model  Accuracy  Precision    Recall        F1\n",
       "0  Hybrid   0.96743     0.9753  0.777815  0.865434"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "model_name = \"Hybrid\"  # change per notebook\n",
    "\n",
    "\n",
    "df = pd.DataFrame([[model_name, acc_05, prec_05, rec_05, f1_05]],\n",
    "                  columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "csv_path = f\"results/{model_name}.csv\"\n",
    "\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Exported metrics to {csv_path}\")\n",
    "\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
