{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "814df1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "X_PATH = \"processed/preprocessed_X_seq.npy\"\n",
    "Y_PATH = \"processed/preprocessed_y_seq.npy\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "LR = 1e-3\n",
    "SEQ_LEN = 32\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097fdb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (78773, 32, 52)\n",
      "y: (78773, 32)\n",
      "55141 11815 11817\n"
     ]
    }
   ],
   "source": [
    "X_seq = np.load(X_PATH)\n",
    "y_seq = np.load(Y_PATH)\n",
    "\n",
    "print(\"X:\", X_seq.shape)\n",
    "print(\"y:\", y_seq.shape)\n",
    "\n",
    "def create_splits(X, y, train_ratio=0.7, val_ratio=0.15):\n",
    "    N = X.shape[0]\n",
    "    n_train = int(N * train_ratio)\n",
    "    n_val = int(N * val_ratio)\n",
    "    return (\n",
    "        (X[:n_train], y[:n_train]),\n",
    "        (X[n_train:n_train+n_val], y[n_train:n_train+n_val]),\n",
    "        (X[n_train+n_val:], y[n_train+n_val:])\n",
    "    )\n",
    "\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = create_splits(X_seq, y_seq)\n",
    "print(len(X_train), len(X_val), len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db9f7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(FlowDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(FlowDataset(X_val, y_val),     batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(FlowDataset(X_test, y_test),   batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f561cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerIDS(nn.Module):\n",
    "    def __init__(self, in_dim, emb_dim=128, n_heads=4, n_layers=2, num_classes=2, max_seq_len=32):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, emb_dim)\n",
    "        )\n",
    "        self.pos = nn.Embedding(max_seq_len, emb_dim)\n",
    "        encoder = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads,\n",
    "                                             dim_feedforward=256, dropout=0.1,\n",
    "                                             batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder, num_layers=n_layers)\n",
    "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, _ = x.shape\n",
    "        x = self.embed(x)\n",
    "        pos = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "        x = x + self.pos(pos)\n",
    "        x = self.encoder(x)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42820ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([0.5757, 3.8037], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "flat = y_train.reshape(-1)\n",
    "counts = np.bincount(flat, minlength=2).astype(float)\n",
    "weights = counts.sum() / (2 * counts + 1e-8)\n",
    "class_weights = torch.tensor(weights, device=DEVICE, dtype=torch.float32)\n",
    "print(\"Class weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14af1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ best model saved at epoch 1 (Val Acc=0.7683)\n",
      "Epoch 1/15 | Train Loss=0.0480 Acc=0.9867 | Val Loss=1.4317 Acc=0.7683\n",
      "Epoch 2/15 | Train Loss=0.0292 Acc=0.9921 | Val Loss=1.9517 Acc=0.6787\n",
      "ðŸ’¾ best model saved at epoch 3 (Val Acc=0.8042)\n",
      "Epoch 3/15 | Train Loss=0.0242 Acc=0.9933 | Val Loss=1.2580 Acc=0.8042\n",
      "Epoch 4/15 | Train Loss=0.0230 Acc=0.9935 | Val Loss=2.0830 Acc=0.6350\n",
      "Epoch 5/15 | Train Loss=0.0246 Acc=0.9932 | Val Loss=1.8103 Acc=0.7241\n",
      "Epoch 6/15 | Train Loss=0.0256 Acc=0.9928 | Val Loss=1.7197 Acc=0.7350\n",
      "Epoch 7/15 | Train Loss=0.0249 Acc=0.9927 | Val Loss=1.3836 Acc=0.7601\n",
      "Epoch 8/15 | Train Loss=0.0253 Acc=0.9925 | Val Loss=2.0902 Acc=0.7336\n",
      "Epoch 9/15 | Train Loss=0.0274 Acc=0.9927 | Val Loss=1.8016 Acc=0.7304\n",
      "Epoch 10/15 | Train Loss=0.0240 Acc=0.9934 | Val Loss=3.5309 Acc=0.5696\n",
      "Epoch 11/15 | Train Loss=0.0252 Acc=0.9930 | Val Loss=2.0854 Acc=0.7076\n",
      "Epoch 12/15 | Train Loss=0.0466 Acc=0.9899 | Val Loss=2.1567 Acc=0.6846\n",
      "Epoch 13/15 | Train Loss=0.0295 Acc=0.9915 | Val Loss=3.2949 Acc=0.5681\n",
      "Epoch 14/15 | Train Loss=0.0246 Acc=0.9920 | Val Loss=1.7012 Acc=0.7669\n",
      "Epoch 15/15 | Train Loss=0.0288 Acc=0.9915 | Val Loss=1.6201 Acc=0.6587\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "model = SimpleTransformerIDS(in_dim=X_seq.shape[-1]).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "best_val_acc = -1\n",
    "best_model = \"results/transformer_best.pth\"\n",
    "\n",
    "def acc_fn(logits, y):\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    return (preds == y).float().mean().item()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    tl, ta = 0, 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits.view(-1,2), yb.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tl += loss.item()\n",
    "        ta += acc_fn(logits, yb)\n",
    "    tl /= len(train_loader); ta /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    vl, va = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits.view(-1,2), yb.view(-1))\n",
    "            vl += loss.item()\n",
    "            va += acc_fn(logits, yb)\n",
    "    vl /= len(val_loader); va /= len(val_loader)\n",
    "\n",
    "    if va > best_val_acc:\n",
    "        best_val_acc = va\n",
    "        torch.save(model.state_dict(), best_model)\n",
    "        print(f\"ðŸ’¾ best model saved at epoch {epoch} (Val Acc={va:.4f})\")\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | Train Loss={tl:.4f} Acc={ta:.4f} | Val Loss={vl:.4f} Acc={va:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66a7e093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Test Metrics:\n",
      "Accuracy : 0.9276386773292714\n",
      "Precision: 0.930288637194008\n",
      "Recall   : 0.5000687393208555\n",
      "F1-score : 0.6504783680559991\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformer</td>\n",
       "      <td>0.927639</td>\n",
       "      <td>0.930289</td>\n",
       "      <td>0.500069</td>\n",
       "      <td>0.650478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model  Accuracy  Precision    Recall        F1\n",
       "0  Transformer  0.927639   0.930289  0.500069  0.650478"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_model))\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        y_true.extend(yb.numpy().reshape(-1))\n",
    "        y_pred.extend(preds.cpu().numpy().reshape(-1))\n",
    "\n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "print(\"Transformer Test Metrics:\")\n",
    "print(\"Accuracy :\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall   :\", rec)\n",
    "print(\"F1-score :\", f1)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([[ \"Transformer\", acc, prec, rec, f1 ]],\n",
    "                  columns=[\"Model\",\"Accuracy\",\"Precision\",\"Recall\",\"F1\"])\n",
    "df.to_csv(\"results/Transformer.csv\", index=False)\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
